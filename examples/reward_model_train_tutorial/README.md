# Reward Model 使用教程

## 概述

Reward Model是一个对大语言模型生成的句子进行判断和打分，用于评估生成式语言模型结果好坏的模型。本教程以GPT2为例，介绍如何训练和评估reward model。

当前支持模型：
* llama2 7B
* baichuan2 7B/13B

奖励模型的适配教程：
* [llama2 7B](https://github.com/mindspore-lab/mindrlhf/blob/master/examples/reward_model_train_tutorial/llama_reward_model_tutorial.md)
